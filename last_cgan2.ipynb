{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# External\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "# Std\n",
    "import os\n",
    "# Local\n",
    "from lib.functions import show_sample_images\n",
    "from CGAN import CGanNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "NET_NAME = \"Last_CGAN_try3\"\n",
    "BATCH_SIZE = 64\n",
    "IMAGE_WIDTH = 64\n",
    "IMAGE_HEIGHT = 64\n",
    "LEARNING_RATE_DISC: float = 0.0002\n",
    "LEARNING_RATE_GAN: float = 0.0004\n",
    "DROPOUT_RATE: float = 0.4\n",
    "DENSE_UNITS=256\n",
    "CONV_LAYERS = 3\n",
    "BATCH_NORM = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "physical_devices = tf.config.list_physical_devices(\"GPU\")\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load flowers dataset\n",
    "(flowers_training_set, flowers_validation_set), flowers_dataset_info = tfds.load(\n",
    "    'oxford_flowers102',\n",
    "    split=['test[:49%]', 'validation'],\n",
    "    with_info=True,\n",
    "    as_supervised=True,\n",
    ")\n",
    "# Get size of datasets from dataset_info\n",
    "flowers_dataset_size = flowers_dataset_info.splits['test[:49%]'].num_examples\n",
    "print(f'Flower dataset size is: {flowers_dataset_size}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load birds dataset\n",
    "(birds_training_set, birds_test_set), birds_dataset_info = tfds.load(\n",
    "    'caltech_birds2010',\n",
    "    split=['train', 'test'],\n",
    "    with_info=True,\n",
    "    as_supervised=True,\n",
    ")\n",
    "# Get size of datasets from dataset_info\n",
    "birds_dataset_size = birds_dataset_info.splits['train'].num_examples\n",
    "print(f'Birds dataset size is: {birds_dataset_size}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load dogs dataset\n",
    "(dogs_training_set, dogs_test_set), dogs_dataset_info = tfds.load(\n",
    "    'stanford_dogs',\n",
    "    split=['train[:25%]', 'test'],\n",
    "    with_info=True,\n",
    "    as_supervised=True,\n",
    ")\n",
    "# Get size of datasets from dataset_info\n",
    "dogs_dataset_size = dogs_dataset_info.splits['train[:25%]'].num_examples\n",
    "print(f'Dogs dataset size is: {dogs_dataset_size}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocessing images\n",
    "def format_image(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = image / 255.0\n",
    "    image = tf.image.resize(image, (IMAGE_WIDTH, IMAGE_HEIGHT))\n",
    "\n",
    "    return image, label\n",
    "flowers_train_batches = flowers_training_set.shuffle(flowers_dataset_size//4).map(format_image)\n",
    "birds_train_batches = birds_training_set.shuffle(birds_dataset_size//4).map(format_image)\n",
    "dogs_train_batches = dogs_training_set.shuffle(dogs_dataset_size//4).map(format_image)\n",
    "\n",
    "# Get labels numbers to names map\n",
    "labels_strings = {\n",
    "    1: \"Kwiat\",\n",
    "    2: \"Ptak\",\n",
    "    3: \"Pies\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Create numpy arrays with images and labels\n",
    "flowers_numpy_train_batches_images = np.array([_[0] for _ in flowers_train_batches])\n",
    "flowers_numpy_train_batches_labels = np.array([1 for _ in flowers_train_batches])\n",
    "birds_numpy_train_batches_images = np.array([_[0] for _ in birds_train_batches])\n",
    "birds_numpy_train_batches_labels = np.array([2 for _ in birds_train_batches])\n",
    "dogs_numpy_train_batches_images = np.array([_[0] for _ in dogs_train_batches])\n",
    "dogs_numpy_train_batches_labels = np.array([3 for _ in dogs_train_batches])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "numpy_train_batches_images = np.concatenate((flowers_numpy_train_batches_images, birds_numpy_train_batches_images, dogs_numpy_train_batches_images))\n",
    "numpy_train_batches_labels = np.concatenate((flowers_numpy_train_batches_labels, birds_numpy_train_batches_labels, dogs_numpy_train_batches_labels))\n",
    "dataset_size = flowers_dataset_size + birds_dataset_size + dogs_dataset_size"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cgan = CGanNet(net_name=NET_NAME,\n",
    "               batch_size=BATCH_SIZE,\n",
    "               image_width=IMAGE_WIDTH,\n",
    "               image_height=IMAGE_HEIGHT,\n",
    "               learning_rate_disc= LEARNING_RATE_DISC,\n",
    "               learning_rate_gan=LEARNING_RATE_GAN,\n",
    "               dropout_rate= DROPOUT_RATE,\n",
    "               generator_dense_units=DENSE_UNITS,\n",
    "               num_conv_layers=CONV_LAYERS,\n",
    "               batch_norm=BATCH_NORM,\n",
    "               number_of_channels=3,\n",
    "               latent_dimension=100,\n",
    "               training_data=numpy_train_batches_images,\n",
    "               labels_data=numpy_train_batches_labels,\n",
    "               number_of_classes=3,\n",
    "               labels_names = labels_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Define GAN models\n",
    "cgan.define_discriminator()\n",
    "cgan.define_generator()\n",
    "cgan.define_gan()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# cgan.train(number_of_epochs=1, load_past_model=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "show_sample_images(numpy_train_batches_images, numpy_train_batches_labels, labels_strings, NET_NAME, dataset_size=dataset_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cgan.visualize_models()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in dataset: 9013\n",
      "Batches per epoch: 140\n",
      "Half batch size is: 32\n",
      "----> Load epoch number: 301 from file /home/kamil/Repositories/GAN_mg/Last_CGAN_try3_data/.epoch\n",
      "------------------------------------------------------------\n",
      "---> Epoch: 302 1/300\n",
      "---> Start time is: 23:47:45\n",
      "140/140 [==============================] - 38s 273ms/step\n",
      "---> End time is: 23:48:24\n",
      "---> D_real_loss: 0.539697527885437 D_fake_loss: 0.1129714697599411 G_loss: 45.89104461669922\n",
      "----> D_real_acc: 0.8125 D_fake_acc: 1.0\n",
      "----> Epoch training time: 0.0h 0.0m, 38s\n",
      "------------------------------------------------------------\n",
      "---> Epoch: 303 2/300\n",
      "---> Start time is: 23:48:24\n",
      "140/140 [==============================] - 41s 293ms/step\n",
      "---> End time is: 23:49:05\n",
      "---> D_real_loss: 0.05779379978775978 D_fake_loss: 0.08798177540302277 G_loss: 68.90676879882812\n",
      "----> D_real_acc: 0.96875 D_fake_acc: 1.0\n",
      "----> Epoch training time: 0.0h 0.0m, 40s\n",
      "------------------------------------------------------------\n",
      "---> Epoch: 304 3/300\n",
      "---> Start time is: 23:49:05\n",
      " 88/140 [=================>............] - ETA: 15s"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [13]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mcgan\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnumber_of_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m300\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mload_past_model\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Repositories/GAN_mg/CGAN.py:243\u001B[0m, in \u001B[0;36mCGanNet.train\u001B[0;34m(self, number_of_epochs, load_past_model)\u001B[0m\n\u001B[1;32m    241\u001B[0m generator_expected_answers \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mones((\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_batch_size, \u001B[38;5;241m1\u001B[39m))\n\u001B[1;32m    242\u001B[0m \u001B[38;5;66;03m# Update the generator via the discriminator's error\u001B[39;00m\n\u001B[0;32m--> 243\u001B[0m generator_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_gan\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_on_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mgenerator_random_input_vector\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels_input\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    244\u001B[0m \u001B[43m                                          \u001B[49m\u001B[43mgenerator_expected_answers\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    245\u001B[0m \u001B[38;5;66;03m# Update progress bar\u001B[39;00m\n\u001B[1;32m    246\u001B[0m progbar\u001B[38;5;241m.\u001B[39mupdate(batch_number \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "cgan.train(number_of_epochs=300, load_past_model=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cgan.get_training_time()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}